{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen =ImageDataGenerator(\n",
    "                                rescale=1/255,\n",
    "                                rotation_range=30,\n",
    "                                shear_range=0.3,\n",
    "                                zoom_range=0.3,\n",
    "                                width_shift_range=0.4,\n",
    "                                height_shift_range=0.4,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28710 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                    './train/',\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    target_size=(48,48),\n",
    "                                                    batch_size=64,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(  \n",
    "                                                    './validation/',\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    target_size=(48,48),\n",
    "                                                    batch_size=64,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(48,48,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(48,48,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(7, kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 741,479\n",
      "Trainable params: 739,687\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"Emotion_Model.h5\",\n",
    "                            monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            save_best_only=True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                         min_delta=0,\n",
    "                         patience=5,\n",
    "                         verbose=1,\n",
    "                         restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=0.2,\n",
    "                             patience=3,\n",
    "                             verbose=1,\n",
    "                             min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-983cccfd4ad0>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 2.2672 - accuracy: 0.1909\n",
      "Epoch 00001: val_loss improved from inf to 1.81516, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 167s 373ms/step - loss: 2.2672 - accuracy: 0.1909 - val_loss: 1.8152 - val_accuracy: 0.2469 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.8621 - accuracy: 0.2278\n",
      "Epoch 00002: val_loss improved from 1.81516 to 1.78297, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 173s 386ms/step - loss: 1.8621 - accuracy: 0.2278 - val_loss: 1.7830 - val_accuracy: 0.2561 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7986 - accuracy: 0.2503\n",
      "Epoch 00003: val_loss improved from 1.78297 to 1.77458, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 175s 389ms/step - loss: 1.7986 - accuracy: 0.2503 - val_loss: 1.7746 - val_accuracy: 0.2630 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7836 - accuracy: 0.2579\n",
      "Epoch 00004: val_loss improved from 1.77458 to 1.75324, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 175s 391ms/step - loss: 1.7836 - accuracy: 0.2579 - val_loss: 1.7532 - val_accuracy: 0.2711 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7582 - accuracy: 0.2746\n",
      "Epoch 00005: val_loss improved from 1.75324 to 1.68833, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 176s 392ms/step - loss: 1.7582 - accuracy: 0.2746 - val_loss: 1.6883 - val_accuracy: 0.3238 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7122 - accuracy: 0.3015\n",
      "Epoch 00006: val_loss improved from 1.68833 to 1.66847, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 176s 392ms/step - loss: 1.7122 - accuracy: 0.3015 - val_loss: 1.6685 - val_accuracy: 0.3525 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.6408 - accuracy: 0.3445\n",
      "Epoch 00007: val_loss improved from 1.66847 to 1.61619, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 177s 394ms/step - loss: 1.6408 - accuracy: 0.3445 - val_loss: 1.6162 - val_accuracy: 0.3823 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.5691 - accuracy: 0.3838\n",
      "Epoch 00008: val_loss did not improve from 1.61619\n",
      "449/449 [==============================] - 177s 394ms/step - loss: 1.5691 - accuracy: 0.3838 - val_loss: 1.6845 - val_accuracy: 0.3826 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.5111 - accuracy: 0.4148\n",
      "Epoch 00009: val_loss improved from 1.61619 to 1.50719, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 176s 392ms/step - loss: 1.5111 - accuracy: 0.4148 - val_loss: 1.5072 - val_accuracy: 0.4352 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4721 - accuracy: 0.4342\n",
      "Epoch 00010: val_loss did not improve from 1.50719\n",
      "449/449 [==============================] - 176s 393ms/step - loss: 1.4721 - accuracy: 0.4342 - val_loss: 1.5431 - val_accuracy: 0.4433 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4300 - accuracy: 0.4502\n",
      "Epoch 00011: val_loss did not improve from 1.50719\n",
      "449/449 [==============================] - 177s 393ms/step - loss: 1.4300 - accuracy: 0.4502 - val_loss: 1.5266 - val_accuracy: 0.4648 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4035 - accuracy: 0.4627\n",
      "Epoch 00012: val_loss did not improve from 1.50719\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "449/449 [==============================] - 177s 394ms/step - loss: 1.4035 - accuracy: 0.4627 - val_loss: 1.5647 - val_accuracy: 0.4483 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3599 - accuracy: 0.4835\n",
      "Epoch 00013: val_loss improved from 1.50719 to 1.46510, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 177s 395ms/step - loss: 1.3599 - accuracy: 0.4835 - val_loss: 1.4651 - val_accuracy: 0.4787 - lr: 2.0000e-04\n",
      "Epoch 14/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3373 - accuracy: 0.4909\n",
      "Epoch 00014: val_loss did not improve from 1.46510\n",
      "449/449 [==============================] - 178s 396ms/step - loss: 1.3373 - accuracy: 0.4909 - val_loss: 1.4686 - val_accuracy: 0.4876 - lr: 2.0000e-04\n",
      "Epoch 15/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3322 - accuracy: 0.4927\n",
      "Epoch 00015: val_loss improved from 1.46510 to 1.45616, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 178s 396ms/step - loss: 1.3322 - accuracy: 0.4927 - val_loss: 1.4562 - val_accuracy: 0.4840 - lr: 2.0000e-04\n",
      "Epoch 16/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3228 - accuracy: 0.5011\n",
      "Epoch 00016: val_loss improved from 1.45616 to 1.42197, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 178s 397ms/step - loss: 1.3228 - accuracy: 0.5011 - val_loss: 1.4220 - val_accuracy: 0.4990 - lr: 2.0000e-04\n",
      "Epoch 17/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3122 - accuracy: 0.5022\n",
      "Epoch 00017: val_loss did not improve from 1.42197\n",
      "449/449 [==============================] - 178s 397ms/step - loss: 1.3122 - accuracy: 0.5022 - val_loss: 1.4403 - val_accuracy: 0.4932 - lr: 2.0000e-04\n",
      "Epoch 18/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3045 - accuracy: 0.5058\n",
      "Epoch 00018: val_loss did not improve from 1.42197\n",
      "449/449 [==============================] - 178s 396ms/step - loss: 1.3045 - accuracy: 0.5058 - val_loss: 1.4540 - val_accuracy: 0.4896 - lr: 2.0000e-04\n",
      "Epoch 19/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2991 - accuracy: 0.5059\n",
      "Epoch 00019: val_loss did not improve from 1.42197\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "449/449 [==============================] - 178s 396ms/step - loss: 1.2991 - accuracy: 0.5059 - val_loss: 1.4445 - val_accuracy: 0.5065 - lr: 2.0000e-04\n",
      "Epoch 20/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2900 - accuracy: 0.5115\n",
      "Epoch 00020: val_loss improved from 1.42197 to 1.42151, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 178s 397ms/step - loss: 1.2900 - accuracy: 0.5115 - val_loss: 1.4215 - val_accuracy: 0.5040 - lr: 4.0000e-05\n",
      "Epoch 21/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2860 - accuracy: 0.5147\n",
      "Epoch 00021: val_loss improved from 1.42151 to 1.42146, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 178s 397ms/step - loss: 1.2860 - accuracy: 0.5147 - val_loss: 1.4215 - val_accuracy: 0.5085 - lr: 4.0000e-05\n",
      "Epoch 22/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2830 - accuracy: 0.5193\n",
      "Epoch 00022: val_loss improved from 1.42146 to 1.41971, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 186s 414ms/step - loss: 1.2830 - accuracy: 0.5193 - val_loss: 1.4197 - val_accuracy: 0.5077 - lr: 4.0000e-05\n",
      "Epoch 23/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2802 - accuracy: 0.5180\n",
      "Epoch 00023: val_loss improved from 1.41971 to 1.41827, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 179s 398ms/step - loss: 1.2802 - accuracy: 0.5180 - val_loss: 1.4183 - val_accuracy: 0.5088 - lr: 4.0000e-05\n",
      "Epoch 24/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2791 - accuracy: 0.5196\n",
      "Epoch 00024: val_loss improved from 1.41827 to 1.41207, saving model to Emotion_Model.h5\n",
      "449/449 [==============================] - 177s 395ms/step - loss: 1.2791 - accuracy: 0.5196 - val_loss: 1.4121 - val_accuracy: 0.5132 - lr: 4.0000e-05\n",
      "Epoch 25/25\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2737 - accuracy: 0.5196\n",
      "Epoch 00025: val_loss did not improve from 1.41207\n",
      "449/449 [==============================] - 200s 445ms/step - loss: 1.2737 - accuracy: 0.5196 - val_loss: 1.4249 - val_accuracy: 0.5102 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=25, steps_per_epoch=449, callbacks=callback, validation_data=validation_generator, validation_steps=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
